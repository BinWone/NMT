INFO:__main__:
Model options:
{'batch_size': 80,
 'beam_size': 10,
 'bleu_script': './data/mteval-v11b.pl',
 'bos_token': '<S>',
 'cache_fine': 1,
 'cache_nhids': 1000,
 'cache_query': ['c_i'],
 'cache_query_dim': 2000,
 'cache_size': 1000,
 'clip_c': 1.0,
 'coverage_dim': 100,
 'coverage_penalty_factor': 0.0,
 'coverage_type': 'neural',
 'decoding_pruning_beam': 3,
 'dropout': 0.5,
 'eos_token': '</S>',
 'finish_after': 20,
 'hook_samples': 3,
 'length_penalty_factor': 0.0,
 'max_fertility': 2,
 'maxout_part': 1,
 'method': 'GRU',
 'nembed_src': 620,
 'nembed_trg': 620,
 'nhids_src': 1000,
 'nhids_trg': 1000,
 'output_kbest': False,
 'reconstruction_weight': 1.0,
 'reload': True,
 'replace_unk': False,
 'res_to_sgm': './data/plain2sgm.py',
 'sample_freq': 50,
 'save_freq': 5000,
 'saveto': './model.npz',
 'saveto_best': './model_best.npz',
 'seq_len_src': 80,
 'seq_len_trg': 80,
 'sort_k_batches': 20,
 'src_vocab_size': 30001,
 'train_cache_parameters_only': False,
 'train_src': './data/train_src',
 'train_trg': './data/train_trg',
 'trg_vocab_size': 30001,
 'unk_dict': './data/unk_dict',
 'unk_id': 1,
 'unk_token': '<UNK>',
 'val_burn_in': 100000,
 'val_burn_in_fine': 150000,
 'valid_freq': 10000,
 'valid_freq_fine': 5000,
 'valid_out': './data/valid_out',
 'valid_src': './data/valid_src',
 'valid_trg': './data/valid_trg',
 'vocab_src': './data/vocab_src.pkl',
 'vocab_trg': './data/vocab_trg.pkl',
 'with_attention': True,
 'with_cache': False,
 'with_context_gate': False,
 'with_coverage': False,
 'with_decoding_pruning': False,
 'with_reconstruction': False}
INFO:__main__:Source Total: 15745 unique words, with a total of 184387 words.
INFO:__main__:Target Total: 12120 unique words, with a total of 202049 words.
INFO:__main__:Source dict contains 30001 words, covers 100.0% of the text
INFO:__main__:Target dict contains 30001 words, covering 100.0% of the text
INFO:__main__:vocab [./data/vocab_src.pkl] and [./data/vocab_trg.pkl] has been dumped
10000 	10000
